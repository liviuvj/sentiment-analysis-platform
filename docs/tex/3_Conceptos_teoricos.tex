\capitulo{3}{Conceptos teóricos}

En las siguientes secciones se detallarán los principales conceptos teóricos que se tratan en este proyecto. No obstante, debido a la naturaleza práctica de este trabajo, el enfoque central se desarrolla en mejor medida en el \autoref{section:relevant_aspects}. Por esta razón, los apartados a continuación expondrán temas relevantes a la construcción de la plataforma \textit{big data} para análisis de sentimientos desarrollada en este proyecto.

\section{Big Data}

La gran cantidad de datos que se genera diariamente a nivel mundial en Internet, y de manera pública, convierten a esta red en una inmensa fuente de información en bruto a la espera de ser procesada y explotada para la obtención de conocimiento.

En la última década, la cantidad de datos generados cada minuto ha aumentado más de un 90\% en algunas plataformas como \textit{YouTube} o \textit{Instagram} (véase \autoref{fig:data-decade}). En la \autoref{fig:data-never-sleeps} se puede observar el gran aumento en el volumen de los datos y la velocidad a la que se generan, teniendo en cuenta también la variedad de los distintos tipos de formato que pueden tomar en cada una de estas plataformas. 

\imagenConFuente{data-decade}{Diferencia entre la cantidad de datos generados por minuto entre 2013 y 2022 de algunas plataformas}{\cite{domoVS}}

\imagenConFuente{data-never-sleeps}{Cantidad de datos generados cada minuto durante el año 2021 en Internet}{\cite{domoInfo}}

A causa de esto, el término \textit{Big Data} se ha convertido en un tema cada vez más recurrente e importante en la tecnología. Se trata de un fenómeno que ha generado la necesidad de evolucionar las técnicas y herramientas tradicionales a otras metodologías capaces no solo de almacenar toda esta cantidad de datos, sino también de explotarla de manera eficiente.

\subsection{Características}

Para entender correctamente qué es el \textit{Big Data}~\cite{oracleData}, es necesario conocer sus características y sus diferencias respecto a los datos normales y corrientes.

\subsubsection{Principales características}

A continuación, se describen las principales características del \textit{Big Data}, comúnmente conocidas como <<Las 3 Vs del \textit{Big Data}>>. 

\begin{itemize}
    \item \textbf{Volumen.} Como se ha comentado anteriormente, una de las mayores diferencias es la gran cantidad de datos que implica. En este sentido, se habla en el nivel de \textit{terabytes} e incluso cientos de \textit{petabytes} de datos. Estos pueden ser recogidos de distintos orígenes, como por ejemplo flujos de acciones o \textit{clicks} de los usuarios en una aplicación web, sensores de equipos industriales en fábricas, monitorización de equipos médicos de alta sensibilidad, etc.  

    \item \textbf{Velocidad.} Otra de las características principales es la velocidad a la que se producen o utilizan estos datos. Los sensores que monitorizan equipos de alta importancia pueden estar generando un flujo continuo de cientos o miles de registros cada segundo, siendo necesario su análisis en tiempo real para evitar consecuencias graves en ciertos escenarios o actuar con la máxima certeza posible.
    
    \item \textbf{Variedad.} Mientras que los datos tradicionales suelen estar estructurados y habituarse en gran parte al esquema de una base de datos relacional, el \textit{Big Data} no suele adecuarse en este ámbito. Los datos son en su mayoría no estructurados o semi-estructurados, con una gran variedad de tipos (texto, audio, vídeo, ...) que pueden presentar poca densidad (lo que quiere decir que son posibles los casos en los que uno o varios atributos estén presentes en algunos registros, pero estén ausentes de casi todos los demás, aún perteneciendo al mismo flujo de datos).
\end{itemize}

\subsubsection{Otras características importantes}

Aparte de las principales propiedades que se asignaron originalmente al \textit{Big Data}, con el tiempo fueron apareciendo más aspectos, hasta llegar a los 42 que existen en la actualidad~\cite{shaferData}. Sin embargo, a continuación se definen otras dos características que resultan de igual importancia que las descritas anteriormente:

\begin{itemize}
    \item \textbf{Valor.} Los datos en bruto no suelen presentar valor aparente por sí mismos. Generalmente, es necesario procesar estos datos y aplicar técnicas de análisis para poder obtener conocimiento útil. Algunas empresas o servicios de popular demanda se basan completamente en los datos de sus usuarios para poder ofrecer un valor añadido. Por ello, resulta necesario que los datos tengan el potencial de generar un valor suficiente. 
    
    \item \textbf{Veracidad.} Otro de los puntos de gran relevancia en el momento actual es trabajar con datos veraces. Resulta de vital importancia conocer qué tan fiables son los datos disponibles para poder asegurar la toma de decisiones futuras a partir de los mismos.
\end{itemize}

\subsection{Desafíos y oportunidades}

Tras entender mejor el concepto \textit{Big Data}, se pueden plantear una serie de desafíos al trabajar con ello que no existían previamente con los datos tradicionales. Los principales retos a superar se pueden deducir a partir de <<Las 5 Vs del \textit{Big Data}>> detalladas anteriormente.

En primer lugar, es necesario disponer de la capacidad de procesar grandes volúmenes de datos en tiempo real. Como se ha indicado previamente, los datos en bruto no suelen presentar valor por sí mismos. Por ello, normalmente se realizan operaciones de integración de datos, en los que se combina información originada de distintas fuentes de datos, tras lo cual se procesan en conjunto y se aplican agregaciones, reglas de negocio o asegurando la calidad de los mismos. Esto ha provocado la necesidad de desarrollar nuevas herramientas capaces de realizar esta labor de procesamiento en tiempo real sobre grandes cantidades de datos.

También surge la necesidad de poder persistir toda la información obtenida de distintos orígenes, teniendo en cuenta las posibles diferencias entre los tipos de datos con los que se trabaja en cada uno de ellos. De esta manera, el \textit{Big Data} ha promovido también el auge de bases de datos no relacionales que puedan ser capaces de soportar tanta variedad de datos.

Resulta de igual relevancia comentar las técnicas con las que se trabajan los datos obtenidos. Distintos tipos de datos requieren de diferentes técnicas de análisis o transformaciones alternativas para poder explotarlos correctamente. La evolución de los métodos tradicionales y la creación de nuevas metodologías de procesamiento de datos resulta de gran ayuda en la obtención de nuevos \textit{insights}.

Finalmente, es fundamental destacar la importancia de la seguridad en el ámbito de los datos, sobre todo al tratar con información de carácter personal. En la actualidad, los datos sobre uno mismo resultan ser lo más valioso que puede poseer una persona, puesto que pueden representar en gran parte la propia identidad. Es crucial el establecimiento de una <<gobernanza de datos>>, de manera que solamente las personas adecuadas puedan tener acceso a información privilegiada, y cada individuo al mínimo necesario para poder llevar a cabo su función.

\section{Proceso ETL}

En secciones anteriores se ha explicado cómo el \textit{Big Data} implica un aumento del volumen, velocidad y variedad de los datos, entre otras características. Lo cual supone también la utilización de distintas fuentes de datos, que suelen incrementar de igual manera en el tiempo y a medida que surgen más requisitos o necesidades para los proyectos.

Para llevar a cabo dicha gestión, se pone en funcionamiento un proceso \textit{ETL}, (\textit{Extract--Transform--Load}). Este proceso tendrá como objetivo dirigir los datos en bruto desde los orígenes de datos hasta los destinos de los mismos, transformándolos por el camino según corresponda para asegurar la obtención de información valiosa y fiable~\cite{databricksETL}.

\subsection{Descripción del proceso}

Como bien se ha indicado, este proceso presenta tres etapas específicas y necesarias. Cada una cuenta con sus propias características y requisitos, lo que lo convierten en uno de los procesos críticos al trabajar con datos. A continuación se detalla el funcionamiento de estas etapas:

\imagenConFuente{etl-process}{Ilustración básica de un proceso \textit{ETL}}{\cite{databricksETL}}

\begin{itemize}
    \item \textbf{Extracción.} La primera etapa del proceso se encarga de realizar la extracción de datos desde las fuentes de datos seleccionadas, \textit{APIs}, bases de datos, registros de sensores, etc. Esta información extraída puede estar formada por distintos tipos de datos y estar tanto en formato estructurado como semi-estructurado o no estructurado. Además, según el funcionamiento del origen de datos, esta extracción podría ser posible realizarla de manera parcial (extrayendo únicamente registros filtrados o los modificados recientemente) o de manera total (extrayendo todos los registros, necesitando posteriormente un identificador o algún método para compararlos con los extraídos previamente y eliminar duplicados).

    \item \textbf{Transformación.} La segunda etapa se centra en el procesamiento de los datos en bruto mediante tareas de limpieza, transformaciones y enriquecimiento de los datos. Tras la finalización de esta parte del proceso, los datos finales obtenidos han de estar correctamente integrados y resultar fiables, almacenándose en un sistema intermedio o de \textit{staging}. Por ello, en este punto deberán estar cumpliendo los requisitos de calidad de datos propuestos y estar disponibles para ser posteriormente utilizados o procesados y enriquecidos en mayor profundidad para fines particulares de la aplicación que los explotará en última instancia. 

    \item \textbf{Carga.} Finalmente, la última etapa del proceso consiste en mover los datos finales, ya fiables y de calidad, al sistema o base de datos de la aplicación que los va a explotar y utilizar.
\end{itemize}

Cabe destacar también el proceso \textit{ELT} (\textit{Extract--Load--Transform})~\cite{ibmETL}, que actúa de manera similar al original \textit{ETL}. En lugar de procesar los datos extraídos y mantenerlos en un área de \textit{staging} para su posterior uso, en el proceso \textit{ELT} los datos se cargan directamente en el sistema o base de datos destino. Por lo que cada aplicación deberá realizar sus propias transformaciones según sea necesario a partir de los datos disponibles tras realizar la carga de los mismos. El proceso \textit{ELT} puede producir mejores resultados en conjuntos de datos no estructurados y de gran volumen.  

\subsection{Desafíos}

La aplicación de un proceso \textit{ETL} o \textit{ELT} se formaliza con la creación de una \textit{data pipeline}, un conjunto de tareas y acciones encargadas de gestionar la totalidad del proceso. El desarrollo y mantenimiento de las \textit{data pipelines} supone numerosos retos a la hora de tratar con grandes volúmenes de datos.

Comenzando por la infraestructura necesaria para ejecutar dichos procesos, los sistemas sobre los que se desarrollan estos flujos de datos han de cumplir con las necesidades específicas de cada proyecto. Distintas tareas pueden necesitar de una serie de recursos distintos, de menor a mayor capacidad computacional o espacio de almacenamiento. Estos sistemas necesitan ser escalables para permitir la flexibilidad de mantener las \textit{data pipelines} disponibles y en correcto estado ante cambios imprevistos en los datos o en los requisitos.

Igualmente y de manera general, al ser cada proyecto distinto y presentar diferentes requerimientos, los desarrollos realizados para un flujo de datos no tienen por qué ser extrapolables a otro proyecto. De esta manera, las aplicaciones que actúan como origen o destino de datos, y las propias herramientas que se encargan del procesamiento de los mismos, pueden estar en continuo desarrollo y evolución. Por consiguiente, surge también la necesidad de un mantenimiento continuo sobre las \textit{data pipelines} creadas para sustentar la correcta fiabilidad de las mismas.

De igual manera, resulta importante destacar la posibilidad de cambios de contexto en los datos. Lo que puede provocar que durante cierta ventana temporal los datos extraídos puedan variar en gran medida respecto a lo establecido por defecto previamente. Esto podría no significar exclusivamente que los nuevos datos sean erróneos, sino que simplemente han modificado sus valores normales. Por ello, también será imprescindible ajustar las métricas de calidad de datos para asegurar que la información obtenida siga siendo fiable.

\section{Big Data Knowledge Discovery}

Un proyecto de esta magnitud,  que resulte modular y escalable a la vez, requiere de una serie de componentes correctamente integrados e interconectados capaces de soportar grandes volúmenes de datos. Por ello, cada fase del proceso ha de estar diseñada con estos objetivos en mente, permitiendo llevar a cabo de manera eficiente las tareas de \textit{knowledge discovery} o <<descubrimiento de conocimiento>> que puedan resultar necesarias para la obtención de valor.

Un antecedente sobre cómo realizar este tipo de interconexión puede darse en la arquitectura \textit{SoMABiT}~\cite{bohlouli2020knowledge} (\textit{Social Media Analysis using Big Data Technology}). Se trata de un concepto de plataforma que permite la ejecución de tareas de integración y análisis de sentimientos. En los siguientes apartados se detalla en mejor medida el concepto y la arquitectura propuesta en dicho artículo.

\subsection{Concepto SoMABiT}

El artículo mencionado desarrolla el concepto \textit{SoMABiT} como una integración de distintas fuentes de datos sobre un sistema \textit{Hadoop HDFS} que sirva como \textit{knowledge base}, sobre el que posteriormente se posibilita la realización de consultas sobre los datos. De esta manera, se consigue crear una herramienta de ayuda a los usuarios para la toma de decisiones.

Los datos extraídos de las fuentes de datos se almacenarían en bases de datos \textit{NoSQL} para su posterior procesamiento y enriquecimiento, persistiéndolos finalmente en la <<base de conocimiento>> \textit{Hadoop}. Finalmente, los usuarios podrían realizar consultas de manera directa sobre este conocimiento, filtrándose los registros que contengan las palabras clave seleccionadas y mostrándose los datos de manera visual en un \textit{dashboard}.

\subsection{Arquitectura SoMABiT}

La arquitectura básica consiste en tres componentes principales, teniendo cada uno una serie de responsabilidades y tareas concretas a llevar a cabo en el conjunto del sistema. En la \autoref{fig:somabit-architecture} se puede observar el diseño empleado por los autores.

\imagenConFuente{somabit-architecture}{Arquitectura básica del sistema \textit{SoMABiT}}{\cite{bohlouli2020knowledge}}

El sistema \textit{SoMABiT} está formado por los siguientes componentes:

\begin{itemize}
    \item \textbf{\textit{SoMABiT--Server}.} Se trata de un clúster de máquinas \textit{Cloudera} que actúan a modo de servidor para la plataforma. Este componente se encarga de recibir las configuraciones de las consultas realizadas por los usuarios y lanzar las peticiones de extracción a los distintos orígenes de datos, almacenando la información obtenida posteriormente en \textit{HBase} y creando las tablas necesarias para ello.

    \item \textbf{\textit{SoMABiT--WebServlet}.} Este servicio se encarga del procesamiento analítico de los datos y de persistir las tareas lanzadas por los usuarios en una base de datos \textit{MySQL}, junto con información sobre la configuración de los trabajos \textit{MapReduce} a realizar y el momento de tiempo en el que ejecutar cada uno. Se comunica con el \textit{SoMABiT--Server} para realizar la extracción de los datos.

    \item \textbf{\textit{Graphical User Interface} (\textit{GUI}).} Interfaz gráfica a la que pueden acceder los usuarios y administradores de la plataforma para interactuar con el servicio \textit{SoMABiT--WebServlet}. Cada tipo de usuario puede realizar distintas acciones según los permisos correspondientes. Por lo que los usuarios finales puede estar accediendo únicamente a los \textit{dashboards} de visualización de datos, mientras que usuarios más técnicos pueden estar encargándose de configurar y lanzar nuevos trabajos sobre los datos.
\end{itemize}

\section{Procesamiento de lenguaje natural}

Las técnicas \textit{NLP} (\textit{Natural Language Processing}) consisten en el estudio, análisis y procesamiento por máquinas del lenguaje natural. Estas tareas se llevan a cabo para que un sistema pueda comprender diversos aspectos sobre los datos y generar así un modelo de la interpretación del lenguaje con el que poder trabajar en mayor medida posteriormente.

No obstante, conseguir tal objetivo no resulta en una tarea nada trivial. Uno de los mayores desafíos en este ámbito es que el modelo empleado consiga comprender la noción del significado de una palabra en combinación con el resto de un predicado.

Por ejemplo, el siguiente enunciado se trata claramente de un sarcasmo, indicando que la persona no ha disfrutado de una obra: <<¡Qué buena comedia, un poco más y me hubiese reído!>>. Sin embargo, algunos modelos \textit{NLP} más simples podrían tratarlo como una buena experiencia por parte de la persona, puesto que incluye palabras como <<buena>>, <<más>> y <<reír>> que, por lo general, se podrían asignar con sentimientos positivos.

En los siguientes apartados se indican las principales técnicas que constituyen el \textit{state-of-the-art} en cuanto a técnicas de procesamiento de lenguaje natural.

\subsection{Principales técnicas del estado del arte}

Las técnicas \textit{NLP} han ido evolucionando a lo largo del tiempo, desde tareas básicas utilizando bolsas de palabras hasta modelos más avanzados basados en \textit{Deep Learning}.

En la \autoref{fig:nlp-timeline} se puede observar la evolución histórica de estas técnicas, culminando con la tendencia en la que se está enfocando actualmente, los llamados modelos preentrenados de lenguaje o \textit{LLM} (\textit{Large Language Models}).

\imagenConFuente{nlp-timeline}{Evolución de las técnicas \textit{NLP}}{\cite{khurana2023natural}}

A continuación, se describen algunas de las principales técnicas que han constituido el estado del arte~\cite{cromoNLP}:

\begin{itemize}
    \item \textbf{\textit{Recurrent Neural Networks}.}\\
    Las redes neuronales recurrentes (\textit{RNN}) son un caso particular de las redes neuronales convencionales. En estos modelos, las capas intermedias completamente conectadas presentan un segundo \textit{output} a diferencia de las clásicas. 
    
    Aparte de realizar la fase \textit{feedforward} en la que se propagan los valores y se realiza el ajuste de pesos por las capas del modelo, la salida de cada capa (tras pasar por las funciones de activación) vuelve a ser utilizada como entrada de la misma en la siguiente iteración. De esta manera, los resultados obtenidos de la aplicación de una \textit{RNN} no dependen solamente de los datos de entrada actuales, sino también de los anteriormente recibidos. 

    \imagenConFuente{rnn-model}{Esquema básico de neuronas recurrentes}{\cite{cromoNLP}}

    \item \textbf{\textit{Long Short-Term Memory Networks}.}\
  
    % \imagenConFuente{lstm-model}{Esquema básico de neuronas recurrentes con funciones de olvido incluidas}{\cite{cromoNLP}}

    \imagenConFuenteConAnchura{0.8}{lstm-model}{Esquema básico de neuronas recurrentes con funciones de olvido incluidas}{\cite{cromoNLP}}

    Estas redes neuronales son una evolución de las \textit{RNN}. Aparte de presentar mejoras en el cálculo de los pesos, la novedad reside en un componente adicional que permite a la red <<olvidar>> algunos valores de lo entrenado previamente.
    
    De este modo se consigue que el modelo siga siendo capaz de aprender de manera continua y, al no <<olvidar>> todo sino solamente alguna parte de lo aprendido, se obtiene también cierta capacidad de memoria.

    \item \textbf{Mecanismos de atención.}\\    
    Este mecanismo forma parte del modelo \textit{Transformer}~\cite{vaswani2017attention}, que utiliza una estructura de \textit{encoder-decoder}. El \textit{encoder} es la parte de la red que se encarga de construir una representación continua a partir de datos de entrada en forma de símbolos para el \textit{decoder}, y este último tiene como objetivo generar una secuencia de símbolos como respuesta a partir de la representación continua creada por el \textit{encoder}.

    \imagenConFuenteConAnchura{0.8}{transformer-model}{Arquitectura del modelo \textit{Transformer}}{\cite{vaswani2017attention}}
    
    Como se puede ver en la \autoref{fig:transformer-model}, el modelo no utiliza \textit{RNNs} ni {LSTMs}, pero presenta estos mecanismos de atención tanto en las capas del \textit{encoder} como del \textit{decoder}. El funcionamiento de estos mecanismos se basa en la idea de que, en lugar de procesar toda la secuencia de entrada de manera uniforme, se asignan pesos a diferentes partes de la secuencia.
    
    Esto permite al modelo poner la atención en las partes más relevantes y útiles para la tarea a realizar. La ausencia de \textit{RNNs} en el modelo también implica la eliminación de las limitaciones que estas suponían, por lo que el modelo \textit{Transformer} se vuelve capaz de mantener una memoria mucho mayor de lo aprendido. 
    
    En la \autoref{fig:attention-mechanism} se pueden observar las operaciones realizadas para para calcular los pesos de atención y combinar los datos de entrada (\textit{query}, \textit{key}, \textit{value}) de entrada de manera ponderada.

    \imagenConFuenteConAnchura{0.9}{attention-mechanism}{Mecanismo de atención del modelo \textit{Transformer}}{\cite{vaswani2017attention}}

    \item \textbf{\textit{BERT}.}\\
    El modelo \textit{BERT} (\textit{Bidirectional Encoder Representations from Transformers}) se ha convertido en la referencia base del \textit{state-of-the-art} actual, a partir del cual se han ido desarrollando otros para tareas concretas.
    
    Se trata de una mejora del \textit{Transformer} en la que, en lugar de tener en cuenta solamente el contexto anterior a cada palabra, se consigue tener en cuenta el contexto posterior también, convirtiéndolo así en un modelo bidireccional.

    No obstante, el mayor impacto que ha tenido \textit{BERT} es su método de entrenamiento. Este modelo se ha entrenado de manera no supervisada intentando predecir de manera correcta la siguiente palabra de una frase. 
    
    Esto se ha conseguido <<enmascarando>> una palabra de cada enunciado del conjunto de datos (por ejemplo, <<He entrado en la <<\textit{[MASK]}>> del banco.>>). Esto permite que el modelo aprenda tanto las representaciones de las palabras como las relaciones entre ellas y su significado el contexto de cada enunciado.

    \imagenConFuente{bert-model}{Entrenamiento y \textit{fine-tuning} del modelo \textit{BERT}. El pre-entrenamiento del \textit{LLM} permite su posterior adaptación mediante transferencia de aprendizaje a nuevas tareas o conjuntos de datos}{\cite{devlin2019bert}}
    
    Por estas razones, se considera un \textit{LLM} (\textit{Large Language Model}) debido a su entrenamiento sobre grandes volúmenes de datos y a su arquitectura, puesto que el modelo \textit{BERT} base cuenta con 12 capas, y 12 mecanismos de atención y 110 millones de parámetros.
    
    Este entrenamiento previo intensivo es de gran ayuda al realizar \textit{transfer learning}, puesto que se puede adaptar a nuevas tareas simplemente añadiendo nuevas capas y realizando un entrenamiento posterior con datos etiquetados para dicha tarea.
   
\end{itemize}

\subsection{Aplicaciones}

Las técnicas de procesamiento de lenguaje natural tienen un amplio catálogo de aplicaciones en el que se pueden utilizar. Estos casos de uso han ido mejorando con el tiempo a medida que han evolucionado también las técnicas mencionadas anteriormente. No obstante, debido a las restricciones tecnológicas o computacionales de algunos proyectos, no siempre resulta posible utilizar los últimos avances en este campo.

A continuación, se listan algunas de las aplicaciones de las técnicas \textit{NLP}:

\begin{itemize}
    \item \textbf{Clasificación de texto.} Consiste en categorizar el texto mediante etiquetas preestablecidas. Algunas de estas tareas pueden ser la clasificación de tópicos de un texto o el análisis de sentimientos, que se centra en determinar aspectos como el sentimiento (positivo, neutro o negativo), el sarcasmo o la actitud emocional (alegre, triste, enfadado) de la persona que ha proporcionado un enunciado~\cite{felbo2017using, kim2014convolutional}.

    \item \textbf{Traducción automática.} Trata la conversión de un texto en un idiota origen a un idioma destino~\cite{wu2016google}.

    \item \textbf{Filtros de \textit{spam}.} Identificación de mensajes y correos electrónicos no deseados~\cite{sharmin2020convolutional}.

    \item \textbf{Extracción de información.} Selección automática de información específica y extracción de fragmentos relevantes~\cite{lample2016neural}.

    \item \textbf{Elaboración de resúmenes.} Generación de la síntesis de un texto de manera concisa y coherente~\cite{see2017get}.

    \item \textbf{Sistemas de diálogo.} Como pueden ser los asistentes virtuales o los \textit{chatbots}, que utilizan el intercambio de lenguaje natural entre máquinas y humanos para realizar acciones específicas~\cite{chao2019bert}.

    \item \textbf{Medicina.} Análisis de registros clínicos, información sanitaria y datos médicos~\cite{chen2019bert}.
\end{itemize}
