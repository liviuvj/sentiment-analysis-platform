\capitulo{6}{Trabajos relacionados} \label{section:related_works}

En este apartado se describirán otras herramientas similares ya existentes que cumplen un propósito similar al planteado en este proyecto. También se escribirá sobre los principales artículos científicos que comprenden el \textit{state-of-the-art} relacionado con las técnicas de procesamiento de lenguaje natural utilizadas.

\section{Herramientas similares}

A continuación se detallan las características de las principales plataformas que existen actualmente que cumplen con funciones similares a las planteadas en este proyecto. Se han categorizado según supongan o no algún coste económico para su utilización.

\subsection{Herramientas de pago}

Comenzando con las opciones de pago, por ser más establecidas y conocidas que las gratuitas.

\subsubsection{Brand24}

Es una plataforma\footnote{\url{https://brand24.com/}} que monitoriza las menciones sobre la marca del cliente tanto en la web como en redes sociales. Utiliza técnicas NLP para analizar en tiempo real los datos de diversas fuentes como blogs, foros, redes sociales, vídeos...

Una de las ventajas competitivas que ofrece es su capacidad de mostrar la influencia que ha tenido cada mención. Como desventaja, cabe destacar el limitado número de menciones que permite monitorizar en sus servicios de suscripción. El rango de precios comprende desde los \$49 mensuales del paquete básico hasta los \$348 del paquete ejecutivo.

\subsubsection{MonkeyLearn}

Es un conjunto de herramientas de análisis de texto que permite crear modelos propios de \textit{machine learning} sobre los datos introducidos, empleando la propia interfaz gráfica de la plataforma.

Como ventaja principal, provee unos modelos ya entrenados que se pueden utilizar en la mayoría de las situaciones, pero permite también entrenarlos sobre los datos específicos que interesen al cliente. Como desventajas, se podrían incluir la manera de establecer la conexión con los datos, puesto que necesita acceso directo a la base de datos del cliente, además de requerir una suscripción mensual de \$299.

\subsubsection{Repustate}

Es una herramienta de análisis\footnote{\url{https://www.repustate.com/}} de sentimientos que analiza de manera sintáctica los datos introducidos para poder evaluar de mejor manera la intención de cada texto. También es capaz de analizar \textit{emojis} según el contexto en el que se utilicen y provee una API que da soporte a 23 idiomas distintos.

Las principales ventajas que ofrece son la gran cantidad de idiomas que soporta y la posibilidad de especificar distintos significados de palabras concretas para mejorar el análisis que realiza. Como principal desventaja, la utilización de este servicio requiere una suscripción mensual de \$199 para su plan \textit{Standard} o \$499 para el \textit{Premium}.

\subsection{Herramientas gratuitas}

A continuación, las opciones que no requieren realizar gasto económico alguno para utilizar sus funcionalidades básicas.

\subsubsection{Social Searcher}

Es una herramienta sencilla\footnote{\url{https://www.social-searcher.com/}} que ofrece búsqueda por palabras clave, etiquetas o usuarios y muestra unos análisis básicos sobre los resultados obtenidos. Muestra un \textit{dashboard} con varias pestañas en las que se realizan distintos tipos de análisis, además de gráficos diversos que categorizan las menciones en temas y clasifican las opiniones de los usuarios.

La principal ventaja de esta herramienta es que permite aprovechar sus servicios de manera gratuita y sin límite de consultas, aunque tenga también planes de pago. Como desventaja, las funcionalidades que ofrece la versión gratuita son bastante básicas. 

\subsubsection{Tweet Sentiment Viz}

Esta herramienta es la más básica\footnote{\url{https://www.csc2.ncsu.edu/faculty/healey/tweet_viz/tweet_app/}} de la lista. Muestra una serie de gráficos exploratorios (temas, mapas de calor, nubes de palabras, etc.) sobre los datos buscados en tiempo real en función de palabras clave.

Como principal ventaja, es que funciona bastante bien dentro de unos límites preestablecidos. Entre sus desventajas, esta herramienta analiza únicamente datos de la plataforma Twitter, además de emplear técnicas de bolsas de palabras. Por lo que tendrá dificultades a la hora de interpretar cualquier palabra utilizada que no esté dentro de dichos diccionarios.

\section{Artículos científicos}

A continuación, se detallan los artículos científicos que más relevancia han tenido en relación a los objetivos establecidos para este proyecto.

\subsection{BERT: Bidirectional Encoder Representations from Transformers}

Se trata de un modelo~\cite{devlin2019bert} que utiliza una red neuronal ya entrenada para generar \textit{word embeddings} que son utilizadas posteriormente como características en modelos \textit{NLP}.

\textit{BERT} se basa en \textit{transformers} (mecanismos de atención que <<aprenden>> correlaciones entre las palabras de un texto). Estos \textit{transformers} presentan dos componentes, un \textit{encoder} que procesa los datos de entrada y un \textit{decoder} que se encarga de realizar las predicciones correspondientes. Sin embargo, como el objetivo es construir un modelo de lenguaje, tan solo hace falta la primera parte de estos, el codificador.

Mientras que los modelos hasta el momento tomaban una dirección de lectura secuencial de los datos (bien de izquierda a derecha o bien al revés), el codificador del \textit{transformer} es capaz de leer cada palabra del texto a la vez. Esto permite al modelo analizar el contexto general en el que se presenta cada palabra y no teniendo en cuenta solamente una dirección. De esta manera, se considera un modelo <<bidireccional>>, aunque en realidad no tenga una dirección como tal.

Generalmente, los modelos de lenguaje se entrenan intentando predecir una secuencia de palabras dentro de un texto, lo que los convierte en unidireccionales. Por ello, \textit{BERT} emplea dos estrategias para mantener su habilidad bidireccional:

\begin{itemize}
    \item \textbf{\textit{Masked LM (MLM)}.} La primera estrategia que se utiliza es ocultar, mediante un \textit{token}, a forma de máscara aproximadamente un 15\% de las palabras del texto de entrada del codificador. Posteriormente, el modelo intentará predecir las palabras que faltan basándose en el contexto que las rodea.
    
    \item \textbf{\textit{Next Sentence Prediction (NSP)}.} La segunda estrategia consiste en entrenar el modelo mediante pares de frases. La mitad de los datos de entrada se divide de tal manera que la segunda frase de cada par es la que va a continuación de la primera frase en el texto original. Mientras que en la otra mitad de los datos, la segunda frase se escoge al azar del texto original. De esta manera, se asume que el modelo será capaz de distinguir correctamente qué frase tiene sentido a continuación de otra. Se utilizan una serie de \textit{tokens} para indicar el inicio y final de cada frase.
\end{itemize}

Ambas estrategias se ponen en práctica y se entrenan a la vez para conseguir minimizar la <<función de pérdida>> o \textit{loss function} del modelo.