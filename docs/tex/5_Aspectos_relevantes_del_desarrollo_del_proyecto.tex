\capitulo{5}{Aspectos relevantes del desarrollo del proyecto} \label{section:relevant_aspects}

Este apartado pretende recoger los aspectos más interesantes del desarrollo del proyecto, además de la experiencia práctica adquirida durante la
realización del mismo con las diversas tecnologías empleadas.

\section{Extracción de datos} \label{section:data_extraction}

Comenzando con la primera etapa del proceso ETL, el objetivo de la extracción de datos consiste en investigar y explotar los posibles recursos disponibles para recoger toda la información necesaria para el proyecto. Consecuentemente, los requisitos fundamentales de esta etapa consistirán en localizar las fuentes de datos a utilizar y emplear las herramientas necesarias para extraer dichos datos.

\subsection{Fuentes de datos}

La información necesaria para conseguir el objetivo de este proyecto está formada por opiniones públicas de personas sobre algún tema o temas en concreto. La manera más sencilla de obtener estos datos es empleando recursos web como foros, blogs y redes sociales. Más concretamente, se ha optado por investigar la disponibilidad de \textit{APIs} públicas de los principales sitios web donde las personas publican sus opiniones.

A continuación, se realiza un pequeño resumen de la información de la que se dispone actualmente sobre las \textit{APIs} de cada plataforma.

\subsubsection{Twitter}

En 2006 se abrió al público la \textit{API REST} \cite{twitterGettingStarted} de Twitter, que actualmente se encuentra ya en su versión \textbf{v2}, aunque coexiste a su vez con algunas partes de la misma aún en la versión \textbf{v1.1} y otras de pago (\textit{Premium v1.1} o \textit{Enterprise}). Está basada en \textit{GraphQL}\footnote{Lenguaje de consultas para \textit{API}s que facilita la gestión de datos y peticiones (\url{https://graphql.org/}).} y devuelve los resultados en formato \textit{JSON}.

Los permisos que se deben asignar son solo de lectura o escritura de contenido. Mientras que el número de peticiones varía en función del \textit{endpoint}, la ventana temporal de restricción se limita a tan solo 15 minutos \cite{twitterRateLimits}. 

Ofrece acceso de lectura, escritura, modificación y borrado de una amplia variedad de recursos, como puede verse en la \autoref{tabla:recursosTwitter}.

\tablaSmallSinColoresConFuente{Recursos disponibles a través de la \textit{API} de Twitter}{\cite{twitterAPIreference}}{@{}p{.3 \textwidth} p{.1 \textwidth} p{.5 \textwidth}@{}}{recursosTwitter}
{\multicolumn{1}{l}{\textbf{Recurso}} & \multicolumn{1}{l}{\textbf{Versión}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Tweets}} & v2 & Operaciones \textit{CRUD}. \\
& v1.1 & \\
& \textit{Premium} & \\
& \textit{Enterprise} & \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Users}} & v2 & Gestión y búsqueda de usuarios \\
& v1.1 & y relaciones entre los mismos. \\
& \textit{Premium} & \\
& \textit{Enterprise} & \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Spaces}} & v2 & Búsqueda de espacios y participantes. \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Direct Messages}} & v1.1 & Envío y respuesta a mensajes directos. \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Lists}} & v2 & Gestión de listas de contactos. \\
& v1.1 & \\ 
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Trends}} & v1.1 & Identificar tendencias por zonas geográficas. \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Media}} & v1.1 & Cargar archivos multimedia. \\
\specialrule{.05em}{.1em}{.1em}
\textbf{\textit{Places}} & v1.1 & Búsqueda de lugares. \\
}



\subsubsection{Facebook}

La \textit{API} de Facebook originalmente utilizaba \textit{FQL} (\textit{Facebook Query Language}) como lenguaje de consulta, parecido a \textit{SQL}. Sin embargo, en 2010 comenzó la migración hacia \textit{Graph API} \cite{facebookGraphAPI}, actualmente en su versión \textbf{v16.0}. Se organiza en función de colecciones, nodos y campos. Un nodo es un objeto único que representa una clase del diccionario de datos en concreto, mientras que los campos son atributos del mismo y una colección comprende un conjunto de nodos. Toda esta información es presentada en formato \textit{JSON}.

Presenta una gran cantidad de permisos \cite{facebookPermissions} que son requeridos para realizar las acciones de gestión, algunos de los cuales es necesario que sean aprobados por Facebook para su uso. Además, el número de peticiones que se pueden realizar se limita a 200 por hora por cada usuario \cite{facebookRateLimits}.

La lista de nodos que presenta la \textit{API} es muy extensa, aunque en la \autoref{tabla:nodosFacebook} se muestran algunos de ellos que podrían resultar útiles para el desarrollo de este proyecto.

\tablaSmallSinColoresConFuente{Muestra de nodos disponibles en \textit{Graph API} de Facebook}{\cite{facebookAPIreference}}{l l}{nodosFacebook}
{\multicolumn{1}{l}{\textbf{Nodo}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Comment}} & Comentarios de los objetos. \\
\textbf{\textit{Link}} & Enlaces compartidos. \\
\textbf{\textit{Group}} & Objeto único de tipo grupo. \\
\textbf{\textit{Likes}} & Lista de personas que han dado \textit{like} a un objeto. \\
\textbf{\textit{Page}} & Información sobre páginas. \\
\textbf{\textit{User}} & Representación de un usuario. \\
}

\subsubsection{Instagram}

Lanzada originalmente en 2014 y actualmente integrada junto a la \textit{Graph API} de Facebook. Dispone de dos versiones, una más básica enfocada solamente al consumo de contenido, y la normal, que permite realizar diversos tipos de acciones sobre la cuenta y llevar a cabo su gestión.

Se dispone de un conjunto de permisos requeridos bastante más reducido que para la \textit{API} de Facebook, aunque el límite de peticiones es el mismo ya que funciona sobre la propia \textit{Graph API}.

Al estar basada también en la misma tecnología, la estructura consta de los mismos elementos mencionados en el apartado anterior. La diferencia serían los nodos principales en los que se distribuye su contenido, como se observa en la \autoref{tabla:nodosInstagram}.

\tablaSmallSinColoresConFuente{Muestra de nodos disponibles en \textit{Graph API} de Instagram}{\cite{instagramAPIreference}}{l l}{nodosInstagram}
{\multicolumn{1}{l}{\textbf{Nodo}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Comment}} & Comentarios de los objetos. \\
\textbf{\textit{Hashtag}} & Representa un \textit{hashtag}. \\
\textbf{\textit{Multimedia}} & Referencia una foto, vídeo, historia o álbum. \\
\textbf{\textit{User}} & La cuenta de un usuario. \\
\textbf{\textit{Page}} & Información sobre páginas. \\
}

\subsubsection{YouTube}

Introducida en el año 2013, actualmente en su versión \textbf{v3}, y permite la integración de funcionalidades de la plataforma, búsqueda de contenido y análisis demográficos. Cada recurso se representa como un objeto \textit{JSON} sobre el que se pueden ejecutar varias acciones.

Respecto a permisos requeridos, no son tan estrictos como por parte de Meta. No obstante, el número de peticiones se calcula en función de las <<unidades>> que consume cada tipo de petición, teniendo un total básico de 10\,000 al día \cite{youtubeRateLimits}.

Cada recurso se representa como un objeto de datos con identificador único. Entre ellos, los más representativos para la realización de este proyecto podrían ser los expuestos en la \autoref{tabla:recursosYouTube}.

\tablaSmallSinColoresConFuente{Recursos disponibles a través de la \textit{API} de YouTube}{\cite{youtubeAPIreference}}{l l}{recursosYouTube}
{\multicolumn{1}{l}{\textbf{Recurso}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Caption}} & Representa los subtítulos de un vídeo. \\
\textbf{\textit{Comment}} & Comentarios de los objetos. \\
\textbf{\textit{Playlist}} & Colección de vídeos accesibles de forma secuencial. \\
\textbf{\textit{Search result}} & Información de una búsqueda que apunta a un objeto. \\
\textbf{\textit{Video}} & Objeto representativo para un vídeo. \\
}

\subsubsection{Reddit}

Esta \textit{API} fue lanzada en 2011, proporcionando acceso y gestión sobre todas las acciones disponibles desde su interfaz web. También la menos restrictiva de las estudiadas en esta sección, aunque no por ello menos trabajada.

Como ventaja respecto al resto, permite realizar hasta 60 peticiones por minuto \cite{redditRateLimits} a través de todos sus \textit{endpoints}.

Los recursos se representan como objetos tipo \textit{JSON}. En la \autoref{tabla:recursosReddit} se pueden observar las principales estructuras de datos.

\tablaSmallSinColoresConFuente{Recursos disponibles a través de la \textit{API} de Reddit}{\cite{redditAPIreference}}{l l}{recursosReddit}
{\multicolumn{1}{l}{\textbf{Recurso}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Comment}} & Comentario de las demás estructuras de datos. \\
\textbf{\textit{Subreddit}} & Representación de un subforo. \\
\textbf{\textit{Message}} & Información sobre mensajes. \\
\textbf{\textit{Account}} & Datos de la cuenta de un usuario. \\
}

\subsection{Método de extracción}

Para realizar la extracción de los datos se comenzó a realizar un prototipo inicial empleando los \textit{API wrappers} mencionados anteriormente (véase la \autoref{section:api_wrappers}). No obstante, debido a las razones ya explicadas en dicho apartado, finalmente se ha optado por utilizar la herramienta Airbyte para realizar esta tarea.

Esta plataforma de código abierto se ha instalado en la máquina local mediante contenedores \textit{docker}, lo que ha facilitado en gran medida su despliegue ya que está compuesta por una arquitectura compleja con varios servicios interconectados entre sí. Cuenta con una interfaz web sencilla que permite realizar la gestión y configuración de fuentes de datos, destinos de datos y conexiones. En la \autoref{fig:airbyte_architecture} se puede observar una visión general de la arquitectura de esta herramienta.

También presenta una \textit{API} propia~\cite{airbyteAPI} desde la que es posible gestionar las configuraciones de dichos recursos sin necesidad de acceder a su interfaz web.

\imagenConFuente{airbyte_architecture}{Visión general de la arquitectura de \textit{Airbyte}}{\cite{airbyteOverview}}

Inicialmente se comenzó utilizando el conector básico para Twitter que ya presentaba esta herramienta. Tras comprobar su funcionamiento y las posibilidades de extracción de datos que ofrecía, resultó no ofrecer los datos suficientes que se esperaba.

Por ello, al tratarse de una herramienta \textit{open-source}, se procedió a realizar un desarrollo propio y modificar el código base de dicho conector. Se ampliaron así las posibilidades de parametrización y extracción de datos que ofrecía, mejorando así su facilidad de uso y extensibilidad de opciones. Dicho conector está disponible para su uso como una imagen \textit{Docker} que se puede agregar como un nuevo conector en Airbyte, disponible en el siguiente enlace: \url{https://hub.docker.com/r/liviuvj/airbyte-source-twitter/tags}.

\imagen{airbyte_twitter_connector_og}{Configuración básica del conector original de Airbyte para Twitter}

En la \autoref{fig:airbyte_twitter_connector_og} se pueden observar las opciones básicas de configuración del conector, mientras que en la \autoref{fig:airbyte_twitter_connector_custom} se puede comprobar la cantidad de opciones extendidas que se han implementado.

Además, se ha conservado el flujo de datos básico que ya presentaba el conector y se ha añadido un flujo de datos avanzado, en el que se permite la extracción de todas las opciones de configuración documentadas en la \textit{API} de Twitter para la ejecución de consultas y peticiones.

El código de dicho desarrollo se puede comprobar en el \textit{Pull Request} realizado al repositorio oficial de la herramienta Airbyte y su correspondiente \textit{issue} documentada, accediendo al siguiente enlace: \url{https://github.com/airbytehq/airbyte/pull/25534}.

\imagen{airbyte_twitter_connector_custom}{Configuración ampliada del conector mejorado de Airbyte para Twitter}

\subsection{Cambios críticos en las APIs investigadas}

Al comienzo del proyecto se decidió utilizar inicialmente la \textit{API} de Twitter para realizar la extracción de datos. Las razones tras esta decisión se basaron en que presenta una mayor facilidad de uso que el resto de las \textit{APIs} mencionadas, además de que la herramienta de extracción Airbyte ya disponía de un conector básico para ello. Consecuentemente, la siguiente fuente de datos que se había planteado utilizar para el proyecto sería la \textit{API} de Reddit, por presentar mayor facilidad de extracción de datos enfocados a temas concretos que Facebook o Instagram.

Después de terminar las tareas de integración y mejora del conector de Airbyte para Twitter se dirigió el enfoque hacia las demás etapas del proyecto, dejando así por finalizada esta parte. Tras la inclusión, despliegue y configuración de la herramienta de orquestación de procesos Apache Airflow, se procedió a la realización de pruebas mediante la ejecución completa de la \textit{pipeline ETL} para comprobar el correcto funcionamiento del proyecto, surgiendo así problemas en la etapa de extracción de datos.

Al comprobar el origen de los errores, se descubrieron los cambios críticos que había sufrido recientemente la \textit{API} de Twitter~\cite{karissa2023}~\cite{stokel2023}. El acceso básico sin coste que existía anteriormente permitía realizar hasta 500\,000 peticiones de manera mensual a una multitud de diversos \textit{endpoints}. Actualmente, el acceso de dicho plan (\textit{Free}) sin coste ha quedado altamente restringido, permitiendo tan solo realizar publicaciones en la propia cuenta del desarrollador. El siguiente plan (\textit{Basic}), que permite el acceso al \textit{endpoint} de búsqueda de \textit{tweets} presenta un coste de \$100 mensuales, limitando a 10\,000 el número de peticiones de lectura que se pueden realizar. El siguiente plan que se aproxima al número original de peticiones es el \textit{Pro}, con un coste de \$5\,000 mensuales y restringiendo el acceso a 300\,000 peticiones de búsqueda.

Al comprobar la siguiente opción investigada inicialmente a utilizar como segunda fuente de datos, se descubrió que la \textit{API} de Reddit también está sufriendo cambios muy restrictivos~\cite{watercutter2023}. Los usuarios y desarrolladores de la plataforma han organizado numerosas protestas~\cite{antonio2023} para intentar evitar estos hechos. El nuevo plan básico y sin coste ofrece entre 10 y 100 peticiones por minuto, dependiendo del tipo de cuenta y acceso concedido.

\subsection{Conjunto de datos de demostración}

Por las razones explicadas en la sección anterior, las mejoras implementadas en el conector de Airbyte para Twitter se han vuelto poco usables. También se ha descartado el desarrollo de un nuevo conector para Reddit que dependa de la baja cadencia de peticiones posibles a realizar.

Teniendo esto en cuenta y el poco margen temporal restante para la finalización del proyecto, se ha decidido investigar el uso de un posible conjunto de datos a modo de demostración de uso del proyecto, ya que resulta poco viable utilizar las \textit{APIs} investigadas.

El conjunto de datos seleccionado finalmente ha sido \textit{Game of Thrones S8 (Twitter)}~\cite{got8}, de la plataforma de \textit{data science} Kaggle\footnote{\url{https://www.kaggle.com}}. Se trata de un \textit{dataset} sobre la serie de televisión del mismo nombre, que el autor recolectó mediante un \textit{script} en el lenguaje R de manera diaria a lo largo de un mes durante el estreno de su octava temporada.

Este conjunto de datos en formato \textit{CSV} presenta más de 760\,000 registros y 88 atributos distintos, con información tanto sobre los usuarios como las publicaciones realizadas por los mismos. Lo que resulta de gran utilidad ya que se puede reconstruir una estructura de datos similar a la del primer \textit{data pipeline} realizado mediante la \textit{API} de Twitter.

Para comprobar de mejor manera los datos disponibles y la usabilidad de los mismos se ha realizado un análisis exploratorio, al que se puede acceder a través del \textit{Jupyter Notebook}\footnote{\url{https://colab.research.google.com/drive/1d_obU9idFqjsDi7ezeFs1CORxTUAgh7V}} que se deja disponible.

La primera parte de dicho análisis consiste en comprobar la posibilidad de crear una estructura de datos parecida a la diseñada originalmente para el flujo de datos de la \textit{API} de Twitter, consiguiendo completar dicho esquema de datos en una gran medida.

La segunda parte se centra en buscar palabras clave que puedan servir a modo de tópico o tema de interés sobre el que se pueda obtener más información mediante el análisis de sentimientos. Para ello, se comprueba el interés de los usuarios sobre diversos personajes y temas, de los que se han escogido los siguientes para formar particiones más pequeñas y manejables del conjuntos de datos total:

\begin{itemize}
    \item \textbf{\textit{dataset\_movie}.} Partición compuesta de 6\,996 registros que contienen la palabra clave <<\textit{movie}>> en la publicación.
    \item \textbf{\textit{dataset\_got}.} Partición compuesta de 414\,955 registros que contienen la palabra clave <<\textit{Game of Thrones}>> en la publicación.
    \item \textbf{\textit{dataset\_season8}.} Partición compuesta de 33\,663 registros que contienen la palabra clave <<\textit{season 8}>> en la publicación.
    \item \textbf{\textit{dataset\_daenerys}.} Partición compuesta de 8\,830 registros que contienen la palabra clave <<\textit{Daenerys}>> en la publicación.
    \item \textbf{\textit{dataset\_jon}.} Partición compuesta de 12\,222 registros que contienen la palabra clave <<\textit{Jon}>> en la publicación.
\end{itemize}

\section{Carga de datos}

Una vez completada la extracción de los datos, resulta necesario persistirlos para su posterior uso. La herramienta seleccionada para realizar esta tarea es \textit{MongoDB}~\cite{mongodbArchitecture}.

Esta base de datos no relacional basada en documentos almacena los datos en un \textit{JSON} optimizado llamado \textit{BSON}. Teniendo en cuenta que los datos extraídos mediante las \textit{APIs} que se han detallado en el apartado anterior se encuentran en su totalidad en formato \textit{JSON}, su carga en esta base de datos resulta íntegra y directa, eliminando cualquier necesidad de transformación intermedia para ser ajustados a un esquema concreto.

\textit{MongoDB} facilita el desarrollo al ofrecer una alta flexibilidad de almacenamiento de documentos no estructurados con diferentes tipos de datos en una misma colección. Esta característica resulta de gran importancia para el caso de uso del proyecto, debido a que las consultas realizadas a los distintos servicios web no siempre van a poder encontrar toda la información solicitada en los parámetros de la petición.

Presenta también capacidades de alta disponibilidad y escalabilidad gracias a la replicación y particionamiento de los datos (\textit{sharding}), cumpliendo con las partes \textit{C} (Consistencia) y \textit{P} (Tolerante a particiones) del \textit{Teorema CAP}.

\section{Transformación de los datos}

Una vez finalizada la carga inicial de los datos en \textit{MongoDB}, se procede con su procesamiento. Para ello, se ha utilizado \textit{Spark} con el lenguaje \textit{Scala}, ya que presenta una gran velocidad de cómputo ideal al trabajar con grandes cantidades de datos.

Esta tarea contempla la lectura de los datos realizando una limpieza inicial de los mismos, de manera que se recogen únicamente los datos necesarios y se elimina la compleja estructura de datos de la que se han extraído. Posteriormente se seleccionan los campos que serán exportados nuevamente a una colección de \textit{MongoDB} de datos limpios.

Estos datos estarán ya listos para servir de entrada a los algoritmos de \textit{sentiment analysis} que serán ejecutados a continuación.

\section{Visualización de los datos}

En esta sección se detallarán los aspectos relevantes de la herramienta seleccionada para la visualización de los datos.

\section{Orquestación de los procesos}

La arquitectura implementada en este proyecto resulta compleja no solamente a nivel de integración de las herramientas empleadas, sino también de la comunicación realizada entre ellas y los procesos que intervienen entre cada una.

Debido a estas razones, surge la necesidad de emplear una herramienta capaz de gestionar todo el flujo de acciones que se lleva a cabo entre los distintos componentes del proyecto. Para lograr este objetivo, se ha seleccionado \textit{Apache Airflow} como orquestador de procesos.

En las siguientes secciones se desarrollan los aspectos que mayor influencia han tenido para la integración de esta etapa.

\subsection{Configuración y despliegue}

La documentación de \textit{Apache Airflow} indica de la disponibilidad de un <<entorno \textit{dockerizado} listo para producción>>, aunque dicha afirmación no resulta del todo cierta. Para la correcta configuración y despliegue de esta herramienta ha resultado necesaria la creación de un \textit{script} que automatice de la manera más abstracta posible para el usuario la parametrización y despliegue iniciales de la plataforma.

\subsection{Acceso seguro al \textit{Docker socket}}

Teniendo en cuenta los objetivos del proyecto sobre la creación de una plataforma segura y autocontenida, se ha realizado el despliegue completo de la plataforma en contenedores \textit{Docker}.

Debido a esto, las etapas de procesamiento e inferencia de los datos se realizan en contenedores <<desechables>>, en el sentido de que solamente resulta necesario que estén activos durante el tiempo necesario para realizar sus operaciones. De esta manera, esta parte del flujo de datos se vuelve más dinámica y eficiente, utilizando únicamente los recursos necesarios durante el tiempo requerido y liberándolos nuevamente tras finalizar dichas tareas.

Para realizar dicho despliegue y borrado dinámico de contenedores, \textit{Apache Airflow} necesita acceso al \textit{docker socket}.

\subsection{Integración y flujos de datos}

La orquestación de los procesos y comunicación entre los numerosos componentes del proyecto ha requerido la creación de distintos flujos de datos utilizando diversas tareas y configuraciones.

Teniendo en cuenta la cantidad de fuentes de datos seleccionadas finalmente para el proyecto, se han diseñado dos \textit{data pipelines}. Un flujo de datos con un \textit{DAG} normal para los datos provenientes de la \textit{API} de Twitter y otro flujo de datos que genera de manera dinámica los \textit{DAGs} necesarios para todos los subconjuntos de datos del \textit{dataset} explicado en anteriores apartados.  
