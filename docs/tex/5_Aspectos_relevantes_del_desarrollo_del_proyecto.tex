\capitulo{5}{Aspectos relevantes del desarrollo del proyecto} \label{section:relevant_aspects}

Este apartado pretende recoger los aspectos más interesantes del desarrollo del proyecto, además de la experiencia práctica adquirida durante la
realización del mismo con las diversas tecnologías empleadas.

\section{Extracción de datos} \label{section:data_extraction}

Comenzando con la primera etapa del proceso \textit{ETL}, el objetivo de la extracción de datos consiste en investigar y explotar los posibles recursos disponibles para recoger toda la información necesaria para el proyecto. Consecuentemente, los requisitos fundamentales de esta etapa consistirán en localizar las fuentes de datos a utilizar y emplear las herramientas necesarias para extraer dichos datos.

\subsection{Fuentes de datos}

La información necesaria para conseguir el objetivo de este proyecto está formada por opiniones públicas de personas sobre algún tema o temas en concreto. La manera más sencilla de obtener estos datos es empleando recursos web como foros, \textit{blogs} y redes sociales. Más concretamente, se ha optado por investigar la disponibilidad de \textit{APIs} públicas de los principales sitios web donde las personas publican sus opiniones.

A continuación, se realiza un pequeño resumen de la información de la que se dispone actualmente sobre las \textit{APIs} de cada plataforma.

\subsubsection{Twitter}

En 2006 se abrió al público la \textit{API REST} \cite{twitterGettingStarted} de Twitter, que actualmente se encuentra ya en su versión \textbf{v2}, aunque coexiste a su vez con algunas partes de la misma aún en la versión \textbf{v1.1} y otras de pago (\textit{Premium v1.1} o \textit{Enterprise}). Está basada en \textit{GraphQL}\footnote{Lenguaje de consultas para \textit{API}s que facilita la gestión de datos y peticiones (\url{https://graphql.org/}).} y devuelve los resultados en formato \textit{JSON}.

Los permisos que se deben asignar son solo de lectura o escritura de contenido. Mientras que el número de peticiones varía en función del \textit{endpoint}, la ventana temporal de restricción se limita a tan solo 15 minutos \cite{twitterRateLimits}. 

Ofrece acceso de lectura, escritura, modificación y borrado de una amplia variedad de recursos, como puede verse en la \autoref{tabla:recursosTwitter}.

\tablaSmallSinColoresConFuente{Recursos disponibles a través de la \textit{API} de Twitter}{\cite{twitterAPIreference}}{@{}p{.3 \textwidth} p{.1 \textwidth} p{.5 \textwidth}@{}}{recursosTwitter}
{\multicolumn{1}{l}{\textbf{Recurso}} & \multicolumn{1}{l}{\textbf{Versión}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Tweets}} & v2 & Operaciones \textit{CRUD}. \\
& v1.1 & \\
& \textit{Premium} & \\
& \textit{Enterprise} & \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Users}} & v2 & Gestión y búsqueda de usuarios \\
& v1.1 & y relaciones entre los mismos. \\
& \textit{Premium} & \\
& \textit{Enterprise} & \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Spaces}} & v2 & Búsqueda de espacios y participantes. \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Direct Messages}} & v1.1 & Envío y respuesta a mensajes directos. \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Lists}} & v2 & Gestión de listas de contactos. \\
& v1.1 & \\ 
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Trends}} & v1.1 & Identificar tendencias por zonas geográficas. \\
\specialrule{.05em}{.05em}{0em}
\textbf{\textit{Media}} & v1.1 & Cargar archivos multimedia. \\
\specialrule{.05em}{.1em}{.1em}
\textbf{\textit{Places}} & v1.1 & Búsqueda de lugares. \\
}



\subsubsection{Facebook}

La \textit{API} de Facebook originalmente utilizaba \textit{FQL} (\textit{Facebook Query Language}) como lenguaje de consulta, parecido a \textit{SQL}. Sin embargo, en 2010 comenzó la migración hacia \textit{Graph API} \cite{facebookGraphAPI}, actualmente en su versión \textbf{v16.0}. Se organiza en función de colecciones, nodos y campos. Un nodo es un objeto único que representa una clase del diccionario de datos en concreto, mientras que los campos son atributos del mismo y una colección comprende un conjunto de nodos. Toda esta información es presentada en formato \textit{JSON}.

Presenta una gran cantidad de permisos \cite{facebookPermissions} que son requeridos para realizar las acciones de gestión, algunos de los cuales es necesario que sean aprobados por Facebook para su uso. Además, el número de peticiones que se pueden realizar se limita a 200 por hora por cada usuario \cite{facebookRateLimits}.

La lista de nodos que presenta la \textit{API} es muy extensa, aunque en la \autoref{tabla:nodosFacebook} se muestran algunos de ellos que podrían resultar útiles para el desarrollo de este proyecto.

\tablaSmallSinColoresConFuente{Muestra de nodos disponibles en \textit{Graph API} de Facebook}{\cite{facebookAPIreference}}{l l}{nodosFacebook}
{\multicolumn{1}{l}{\textbf{Nodo}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Comment}} & Comentarios de los objetos. \\
\textbf{\textit{Link}} & Enlaces compartidos. \\
\textbf{\textit{Group}} & Objeto único de tipo grupo. \\
\textbf{\textit{Likes}} & Lista de personas que han dado \textit{like} a un objeto. \\
\textbf{\textit{Page}} & Información sobre páginas. \\
\textbf{\textit{User}} & Representación de un usuario. \\
}

\subsubsection{Instagram}

Lanzada originalmente en 2014 y actualmente integrada junto a la \textit{Graph API} de Facebook. Dispone de dos versiones, una más básica enfocada solamente al consumo de contenido, y la normal, que permite realizar diversos tipos de acciones sobre la cuenta y llevar a cabo su gestión.

Se dispone de un conjunto de permisos requeridos bastante más reducido que para la \textit{API} de Facebook, aunque el límite de peticiones es el mismo ya que funciona sobre la propia \textit{Graph API}.

Al estar basada también en la misma tecnología, la estructura consta de los mismos elementos mencionados en el apartado anterior. La diferencia serían los nodos principales en los que se distribuye su contenido, como se observa en la \autoref{tabla:nodosInstagram}.

\tablaSmallSinColoresConFuente{Muestra de nodos disponibles en \textit{Graph API} de Instagram}{\cite{instagramAPIreference}}{l l}{nodosInstagram}
{\multicolumn{1}{l}{\textbf{Nodo}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Comment}} & Comentarios de los objetos. \\
\textbf{\textit{Hashtag}} & Representa un \textit{hashtag}. \\
\textbf{\textit{Multimedia}} & Referencia una foto, vídeo, historia o álbum. \\
\textbf{\textit{User}} & La cuenta de un usuario. \\
\textbf{\textit{Page}} & Información sobre páginas. \\
}

\subsubsection{YouTube}

Introducida en el año 2013, actualmente en su versión \textbf{v3}, y permite la integración de funcionalidades de la plataforma, búsqueda de contenido y análisis demográficos. Cada recurso se representa como un objeto \textit{JSON} sobre el que se pueden ejecutar varias acciones.

Respecto a permisos requeridos, no son tan estrictos como por parte de Meta. No obstante, el número de peticiones se calcula en función de las <<unidades>> que consume cada tipo de petición, teniendo un total básico de 10\,000 al día \cite{youtubeRateLimits}.

Cada recurso se representa como un objeto de datos con identificador único. Entre ellos, los más representativos para la realización de este proyecto podrían ser los expuestos en la \autoref{tabla:recursosYouTube}.

\tablaSmallSinColoresConFuente{Recursos disponibles a través de la \textit{API} de YouTube}{\cite{youtubeAPIreference}}{l l}{recursosYouTube}
{\multicolumn{1}{l}{\textbf{Recurso}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Caption}} & Representa los subtítulos de un vídeo. \\
\textbf{\textit{Comment}} & Comentarios de los objetos. \\
\textbf{\textit{Playlist}} & Colección de vídeos accesibles de forma secuencial. \\
\textbf{\textit{Search result}} & Información de una búsqueda que apunta a un objeto. \\
\textbf{\textit{Video}} & Objeto representativo para un vídeo. \\
}

\subsubsection{Reddit}

Esta \textit{API} fue lanzada en 2011, proporcionando acceso y gestión sobre todas las acciones disponibles desde su interfaz web. También la menos restrictiva de las estudiadas en esta sección, aunque no por ello menos trabajada.

Como ventaja respecto al resto, permite realizar hasta 60 peticiones por minuto \cite{redditRateLimits} a través de todos sus \textit{endpoints}.

Los recursos se representan como objetos tipo \textit{JSON}. En la \autoref{tabla:recursosReddit} se pueden observar las principales estructuras de datos.

\tablaSmallSinColoresConFuente{Recursos disponibles a través de la \textit{API} de Reddit}{\cite{redditAPIreference}}{l l}{recursosReddit}
{\multicolumn{1}{l}{\textbf{Recurso}} & \multicolumn{1}{l}{\textbf{Descripción}}\\}{
\textbf{\textit{Comment}} & Comentario de las demás estructuras de datos. \\
\textbf{\textit{Subreddit}} & Representación de un subforo. \\
\textbf{\textit{Message}} & Información sobre mensajes. \\
\textbf{\textit{Account}} & Datos de la cuenta de un usuario. \\
}

\subsection{Método de extracción}

Para realizar la extracción de los datos se comenzó a realizar un prototipo inicial empleando los \textit{API wrappers} mencionados anteriormente (véase la \autoref{section:api_wrappers}). No obstante, debido a las razones ya explicadas en dicho apartado, finalmente se ha optado por utilizar la herramienta Airbyte para realizar esta tarea.

Esta plataforma de código abierto se ha instalado en la máquina local mediante contenedores \textit{docker}, lo que ha facilitado en gran medida su despliegue ya que está compuesta por una arquitectura compleja con varios servicios interconectados entre sí. Cuenta con una interfaz web sencilla que permite realizar la gestión y configuración de fuentes de datos, destinos de datos y conexiones. En la \autoref{fig:airbyte_architecture} se puede observar una visión general de la arquitectura de esta herramienta.

También presenta una \textit{API} propia~\cite{airbyteAPI} desde la que es posible gestionar las configuraciones de dichos recursos sin necesidad de acceder a su interfaz web.

\imagenConFuente{airbyte_architecture}{Visión general de la arquitectura de \textit{Airbyte}}{\cite{airbyteOverview}}

Inicialmente se comenzó utilizando el conector básico para Twitter que ya presentaba esta herramienta. Tras comprobar su funcionamiento y las posibilidades de extracción de datos que ofrecía, resultó no ofrecer los datos suficientes que se esperaba.

Por ello, al tratarse de una herramienta \textit{open-source}, se procedió a realizar un desarrollo propio y modificar el código base de dicho conector. Se ampliaron así las posibilidades de parametrización y extracción de datos que ofrecía, mejorando así su facilidad de uso y extensibilidad de opciones. Dicho conector está disponible para su uso como una imagen \textit{Docker} que se puede agregar como un nuevo conector en Airbyte, disponible en el siguiente enlace: \url{https://hub.docker.com/r/liviuvj/airbyte-source-twitter/tags}.

\imagen{airbyte_twitter_connector_og}{Configuración básica del conector original de Airbyte para Twitter}

En la \autoref{fig:airbyte_twitter_connector_og} se pueden observar las opciones básicas de configuración del conector, mientras que en la \autoref{fig:airbyte_twitter_connector_custom} se puede comprobar la cantidad de opciones extendidas que se han implementado.

Además, se ha conservado el flujo de datos básico que ya presentaba el conector y se ha añadido un flujo de datos avanzado, en el que se permite la extracción de todas las opciones de configuración documentadas en la \textit{API} de Twitter para la ejecución de consultas y peticiones.

El código de dicho desarrollo se puede comprobar en el \textit{Pull Request} realizado al repositorio oficial de la herramienta Airbyte y su correspondiente \textit{issue} documentada, accediendo al siguiente enlace: \url{https://github.com/airbytehq/airbyte/pull/25534}.

\imagen{airbyte_twitter_connector_custom}{Configuración ampliada del conector mejorado de Airbyte para Twitter}

\subsection{Cambios críticos en las APIs investigadas}

Al comienzo del proyecto se decidió utilizar inicialmente la \textit{API} de Twitter para realizar la extracción de datos. Las razones tras esta decisión se basaron en que presenta una mayor facilidad de uso que el resto de las \textit{APIs} mencionadas, además de que la herramienta de extracción Airbyte ya disponía de un conector básico para ello. Consecuentemente, la siguiente fuente de datos que se había planteado utilizar para el proyecto sería la \textit{API} de Reddit, por presentar mayor facilidad de extracción de datos enfocados a temas concretos que Facebook o Instagram.

 Después de terminar las tareas de integración y mejora del conector de Airbyte para Twitter se dirigió el enfoque hacia las demás etapas del proyecto, dejando así por finalizada esta parte. Tras la inclusión, despliegue y configuración de la herramienta de orquestación de procesos Apache Airflow, se procedió a la realización de pruebas mediante la ejecución completa de la \textit{pipeline ETL} para comprobar el correcto funcionamiento del proyecto, surgiendo así problemas en la etapa de extracción de datos.

Al comprobar el origen de los errores, se descubrieron los cambios críticos que había sufrido recientemente la \textit{API} de Twitter~\cite{karissa2023, stokel2023}. El acceso básico sin coste que existía anteriormente permitía realizar hasta 500\,000 peticiones de manera mensual a una multitud de diversos \textit{endpoints}. Actualmente, el acceso de dicho plan (\textit{Free}) sin coste ha quedado altamente restringido, permitiendo tan solo realizar publicaciones en la propia cuenta del desarrollador. El siguiente plan (\textit{Basic}), que permite el acceso al \textit{endpoint} de búsqueda de \textit{tweets} presenta un coste de \$100 mensuales, limitando a 10\,000 el número de peticiones de lectura que se pueden realizar. El siguiente plan que se aproxima al número original de peticiones es el \textit{Pro}, con un coste de \$5\,000 mensuales y restringiendo el acceso a 300\,000 peticiones de búsqueda.

Al comprobar la siguiente opción investigada inicialmente a utilizar como segunda fuente de datos, se descubrió que la \textit{API} de Reddit también está sufriendo cambios muy restrictivos~\cite{watercutter2023}. Los usuarios y desarrolladores de la plataforma han organizado numerosas protestas~\cite{antonio2023} para intentar evitar estos hechos. El nuevo plan básico y sin coste ofrece entre 10 y 100 peticiones por minuto, dependiendo del tipo de cuenta y acceso concedido.

Estos cambios críticos en las \textit{APIs} fueron detectados \textbf{a lo largo del \textit{Sprint} 10}. Por lo que fue necesaria la toma de una decisión rápida sobre la manera de proseguir con esta parte del proyecto.

\subsection{Conjunto de datos de demostración}

Por las razones explicadas en la sección anterior, las mejoras implementadas en el conector de Airbyte para Twitter se han vuelto poco usables. También se ha descartado el desarrollo de un nuevo conector para Reddit que dependa de la baja cadencia de peticiones posibles a realizar.

Teniendo esto en cuenta y el poco margen temporal restante para la finalización del proyecto, se ha decidido investigar el uso de un posible conjunto de datos a modo de demostración de uso del proyecto, ya que resulta poco viable utilizar las \textit{APIs} investigadas.

El conjunto de datos seleccionado finalmente ha sido \textit{Game of Thrones S8 (Twitter)}~\cite{got8}, de la plataforma de \textit{data science} Kaggle\footnote{\url{https://www.kaggle.com}}. Se trata de un \textit{dataset} sobre la serie de televisión del mismo nombre, que el autor recolectó mediante un \textit{script} en el lenguaje R de manera diaria a lo largo de un mes durante el estreno de su octava temporada.

Este conjunto de datos en formato \textit{CSV} presenta más de 760\,000 registros y 88 atributos distintos, con información tanto sobre los usuarios, como las publicaciones realizadas por los mismos. Lo que resulta de gran utilidad ya que se puede reconstruir una estructura de datos similar a la del primer \textit{data pipeline} realizado mediante la \textit{API} de Twitter.

No obstante, dichos datos son del año 2019 mientras que los obtenidos mediante la \textit{API} de Twitter datan de este año 2023. Al momento de visualizar los datos habrá grandes discrepancias en las gráficas que incluyan un eje temporal (véase la \autoref{fig:date-data-diff}), por lo que estos datos extraídos de la \textit{API} de Twitter no van a dejarse disponibles para su uso, empleando solamente este conjunto de datos investigado a modo de demostración.

\imagen{date-data-diff}{Discrepancia en el eje temporal al utilizar los datos recientes de la \textit{API} de TWitter y los del conjunto de datos de demostración}

Para comprobar de mejor manera los datos disponibles y la usabilidad de los mismos se ha realizado un análisis exploratorio, al que se puede acceder a través del \textit{Jupyter Notebook}\footnote{\url{https://colab.research.google.com/drive/1d_obU9idFqjsDi7ezeFs1CORxTUAgh7V\#offline=true&sandboxMode=true}} que se deja disponible.

La primera parte de dicho análisis consiste en comprobar la posibilidad de crear una estructura de datos parecida a la diseñada originalmente para el flujo de datos de la \textit{API} de Twitter, consiguiendo completar dicho esquema de datos en una gran medida.

La segunda parte se centra en buscar palabras clave que puedan servir a modo de tópico o tema de interés sobre el que se pueda obtener más información mediante el análisis de sentimientos. Para ello, se comprueba el interés de los usuarios sobre diversos personajes y temas, de los que se han escogido los siguientes para formar particiones más pequeñas y manejables del conjuntos de datos total:

\begin{itemize}
    \item \textbf{\texttt{dataset\_movie}.} Partición compuesta de 6\,996 registros que contienen la palabra clave <<\textit{movie}>> en la publicación.
    \item \textbf{\texttt{dataset\_got}.} Partición compuesta de 414\,955 registros que contienen la palabra clave <<\textit{Game of Thrones}>> en la publicación.
    \item \textbf{\texttt{dataset\_season8}.} Partición compuesta de 33\,663 registros que contienen la palabra clave <<\textit{season 8}>> en la publicación.
    \item \textbf{\texttt{dataset\_daenerys}.} Partición compuesta de 8\,830 registros que contienen la palabra clave <<\textit{Daenerys}>> en la publicación.
    \item \textbf{\texttt{dataset\_jon}.} Partición compuesta de 12\,222 registros que contienen la palabra clave <<\textit{Jon}>> en la publicación.
\end{itemize}

\section{Carga de datos}

La carga de los datos se ha realizado en dos bases de datos distintas para diferentes propósitos. La primera se ha utilizado a modo de \textit{Data Lake} del proyecto para guardar los datos en bruto extraídos, mientras que la segunda se ha empleado como un \textit{Data Warehouse} para realizar la agregación de datos de distintos orígenes.

\subsection{MongoDB}

Una vez completada la extracción de los datos, resulta necesario persistirlos para su posterior uso. La herramienta seleccionada para realizar esta tarea es \textit{MongoDB}~\cite{mongodbArchitecture}.

Esta base de datos no relacional basada en documentos almacena los datos en un \textit{JSON} optimizado llamado \textit{BSON}. Teniendo en cuenta que los datos extraídos mediante las \textit{APIs} que se han detallado en el apartado anterior se encuentran en su totalidad en formato \textit{JSON}, su carga en esta base de datos resulta íntegra y directa, eliminando cualquier necesidad de transformación intermedia para ser ajustados a un esquema concreto.

\textit{MongoDB} facilita el desarrollo al ofrecer una alta flexibilidad de almacenamiento de documentos no estructurados con diferentes tipos de datos en una misma colección. Esta característica resulta de gran importancia para el caso de uso del proyecto, debido a que las consultas realizadas a los distintos servicios web no siempre van a poder encontrar toda la información solicitada en los parámetros de la petición.

Presenta también capacidades de alta disponibilidad y escalabilidad gracias a la replicación y particionamiento de los datos (\textit{sharding}), cumpliendo con las partes \textit{C} (Consistencia) y \textit{P} (Tolerante a particiones) del \textit{Teorema CAP}.

Para más información sobre los esquemas de datos utilizados, consultar el apartado correspondiente de la \autoref{section:mongodb_schema}.

\subsection{ClickHouse}

Tras la ejecución de los modelos de \textit{NLP} sobre los datos procesados, estos resultados son almacenados y agregados en el sistema \textit{OLAP} (\textit{On-Line Analytical Processing}) ClickHouse~\cite{clickhouseFeatures}. Este sistema gestor de bases de datos (SGBD) analítico y columnar permite realizar consultas rápidas sobre grandes volúmenes de datos.

Los datos con los que se trabaja en el proyecto son peticiones \textit{API} cuya respuesta contiene una serie de campos que vienen o no completos según la disponibilidad de los datos. En la mayoría de los casos, algunos de los campos presentan poca densidad de información, por lo que en gran parte de registros estos campos estarán vacíos. El modelo de datos columnar que emplea ClickHouse es perfecto para este tipo de casos en los que algunos campos puedan presentar poca densidad.

Al realizar las consultas a la base de datos, se filtran únicamente los campos o columnas necesarias para resolver la petición en lugar de iterar sobre todos los campos de cada fila como en una base de datos tradicional. Por consiguiente, ClickHouse ofrece un alto rendimiento para este tipo de casos ya que se consigue aumentar notablemente la velocidad de las consultas ejecutadas al seleccionar únicamente los campos que intervienen en ellas.

ClickHouse también presenta escalabilidad horizontal, permitiendo así adaptarse ante cargas de trabajo con \textit{Big Data}. Además cuenta con una alta flexibilidad, admitiendo datos en diversos formatos y empleando motores de almacenamiento con funciones específicas por cada tabla, según resulte necesario.

Otro punto ventajoso a la hora de trabajar con este SGBD es la capacidad de integración que presenta. Ofrece interfaz directa con la herramienta de visualización empleada Apache Superset y presenta una sintaxis \textit{SQL} nativa, por lo que resulta de gran facilidad a la hora de lanzar consultas contra los datos.

Para más información sobre los esquemas de datos utilizados, consultar el apartado correspondiente de la \autoref{section:clickhouse_schema}.


\section{Transformación de los datos}

Una vez finalizada la carga inicial de los datos en \textit{MongoDB}, se procede con su procesamiento. Para esta labor, se han empleado dos herramientas descritas en los siguientes apartados.

\subsection{Apache Spark}

Para la primera fase de la etapa de transformación de datos se ha utilizado \textit{Apache Spark}~\cite{apacheSpark} con el lenguaje de programación \textit{Scala}, ya que permite una gran velocidad de cómputo ideal al trabajar con grandes cantidades de datos. Además presenta las características perfectas para \textit{Big Data}, su arquitectura es escalable y el procesamiento se puede realizar de manera distribuida.

Las tareas de procesamiento contemplan la lectura de los datos en bruto y una posterior limpieza inicial de los mismos. De esta manera, se recogen únicamente los datos necesarios y se elimina la compleja estructura de datos de la que se han extraído. A continuación, se seleccionan los campos que serán exportados nuevamente a una colección de \textit{MongoDB} de datos limpios. Los esquemas de datos antes y después de este procesamiento se pueden comprobar en la \autoref{section:mongodb_schema}.

Estos datos procesados estarán ya listos para servir de entrada a los algoritmos de \textit{sentiment analysis} que serán ejecutados a continuación.

\subsection{HuggingFace Transformers}

Para la segunda fase de la etapa de transformación de datos se ha utilizado la librería \textit{HuggingFace Transformers}~\cite{huggingfaceTransformers} del lenguaje de programación \textit{Python}. Esta librería ha facilitado la ejecución de las técnicas de procesamiento de lenguaje natural gracias a su interfaz unificada para la carga y ejecución de \textit{LLM} (\textit{Large Language Models}) pre-entrenados.

Más concretamente, se han enriquecido los datos procesados gracias a la inferencia de varias tareas mediante variaciones del modelo \textit{BERT} ya explicado en la \autoref{section:theory_nlp}. A continuación, se describen las distintas tareas \textit{NLP} empleadas y sus respectivos modelos empleados:

\begin{itemize}
    \item \textbf{\textit{Sentiment Analysis}.} Consiste en identificar y extraer la opinión o actitud de una persona o entidad hacia un tema. Las posibles categorías de sentimiento son: positivo, neutro o negativo. El modelo empleado: \texttt{Twitter-roBERTa-base for Sentiment Analysis}~\cite{cardiffnlpSentiment,  camacho2022tweetnlp, loureiro2022timelms}.

    \item \textbf{\textit{Emotion Analysis}.} Consiste en reconocer y clasificar las emociones expresadas por una persona en un texto. Las posibles categorías de emoción son: optimismo, alegría, tristeza, enfado. El modelo empleado: \texttt{Twitter-roBERTa-base for Emotion Recognition}~\cite{cardiffnlpEmotion, camacho2022tweetnlp, loureiro2022timelms}

    \item \textbf{\textit{Topic Classification}.} Consiste en clasificar en una o más categorías a un enunciado según su contenido y contexto. El modelo empleado: \texttt{tweet-topic-21-multi}~\cite{cardiffnlpTopic, antypas2022twitter}. Las posibles categorías son las mostradas a continuación:
    \begin{verbatim}
    0: arts_&_culture           1: business_&_entrepreneurs
    2: celebrity_&_pop_culture  3: diaries_&_daily_life
    4: family                   5: fashion_&_style
    6: film_tv_&_video          7: fitness_&_health
    8: food_&_dining            9: gaming
    10: learning_&_educational  11: music
    12: news_&_social_concern   13: other_hobbies
    14: relationships           15: science_&_technology
    16: sports                  17: travel_&_adventure
    18: youth_&_student_life		
    \end{verbatim}

    \item \textbf{\textit{Named Entity Recognition}.} Consiste en localizar y etiquetar las entidades nombradas que aparecen en un texto, como personas, lugares, organizaciones, etc. El modelo empleado:\\\texttt{tner/twitter-roberta-base-dec2021-tweetner7-all}~\cite{tnerNER, ushio2022t, ushio2022named}. Las posibles categorías de entidad son las mostradas a continuación:
    \begin{verbatim}
    0: corporation      1: creative_work    2: event
    3: group            4: location         5: person
    6: product
    \end{verbatim}
    
\end{itemize}


\section{Visualización de los datos}

En esta sección se detallarán los aspectos relevantes de la herramienta seleccionada para la visualización de los datos. Apache Superset~\cite{apacheSuperset} permite a la exploración y visualización de datos, además de compartir información de forma interactiva y colaborativa.

Presenta capacidades de creación de usuarios y permisos, por lo que es posible designar roles específicos para cada usuario según las necesidades del mismo. De esta manera, se consigue implementar cierta seguridad en los datos, de modo que ciertos usuarios podrían interactuar solamente con ciertos \textit{dashboards} según el nivel de acceso que tengan, en caso de trabajar con datos sensibles.

Además, cuenta con una sección llamada <<\textit{SQL Lab}>> en la que es posible realizar consultas en formato \textit{SQL} nativo sobre los distintos conjuntos de datos cargados. Una vez ejecutadas las consultas, estas se pueden exportar a un \textit{dataset} virtual que se puede tomar como base para la creación de nuevas visualizaciones.

En este proyecto se ha desarrollado un cuadro de mando integral que comprende 5 vistas a distinto nivel de detalle sobre los datos recogidos. Para ello, se han empleado las visualizaciones pertinentes junto a una paleta de colores elegida intencionalmente para representar el análisis de sentimientos.

Las vistas del \textit{dashboard} permiten la obtención de \textit{insights} no solo en el ámbito general mediante métricas sobre los datos extraídos, sino también sobre las estadísticas representadas de las publicaciones realizadas y los usuarios de las mismas, junto a la información enriquecida añadida mediante las técnicas \textit{NLP}. 

Para utilizar los datos almacenados en el sistema \textit{OLAP} ClickHouse ha hecho falta la instalación de un conector adicional. Esta configuración, así como el cuadro de mando desarrollado y la importación de los \textit{datasets} desde ClickHouse, se realiza mediante un \textit{script} automático para facilitar el despliegue para el usuario. 

El diseño e implementación de las vistas para el \textit{dashboard} se pueden comprobar en mayor detalle en la \autoref{section:dashboard-design}.


\section{Orquestación de los procesos}

La arquitectura implementada en este proyecto resulta compleja, no solamente a nivel de integración de las herramientas empleadas, sino también de la comunicación realizada entre ellas y los procesos que intervienen entre cada una.

Debido a estas razones, surge la necesidad de emplear una herramienta capaz de gestionar todo el flujo de acciones que se lleva a cabo entre los distintos componentes del proyecto. Para lograr este objetivo, se ha seleccionado \textit{Apache Airflow}~\cite{apacheAirflow} como orquestador de procesos.

En las siguientes secciones se desarrollan los aspectos que mayor influencia han tenido para la integración de esta etapa.

\subsection{Configuración y despliegue}

La documentación de \textit{Apache Airflow} indica de la disponibilidad de un <<entorno \textit{dockerizado} listo para producción>>, aunque dicha afirmación no resulta del todo cierta. Para la correcta configuración y despliegue de esta herramienta ha resultado necesaria la creación de un \textit{script} que automatice de la manera más abstracta posible para el usuario la parametrización y despliegue iniciales de la plataforma.

Este \textit{script} se encarga de la declaración de las variables y conexiones a las demás herramientas necesarias para la ejecución de las \textit{data pipelines} creadas. También crea y configura la clave de encriptación \textit{Fernet}~\cite{fernetKey} y realiza la instalación de los \textit{plugins} empleados (\texttt{apache-airflow-providers-docker}~\cite{airflowDocker} y \texttt{apache-airflow-providers-airbyte}~\cite{airflowAirbyte}), que se han utilizado para, respectivamente, realizar la integración con el servicio \textit{Docker} y ejecutar la sincronización de datos en Airbyte.

\subsection{Acceso seguro al \textit{Docker socket}}

Teniendo en cuenta los objetivos del proyecto sobre la creación de una plataforma segura y autocontenida, se ha realizado el despliegue completo de la plataforma en contenedores \textit{Docker}.

Debido a esto, las etapas de procesamiento e inferencia de los datos se realizan en contenedores <<desechables>>, en el sentido de que solamente resulta necesario que estén activos durante el tiempo necesario para realizar sus operaciones. De esta manera, esta parte del flujo de datos se vuelve más dinámica y eficiente, utilizando únicamente los recursos necesarios durante el tiempo requerido y liberándolos nuevamente tras finalizar dichas tareas.

Para realizar dicho despliegue y borrado dinámico de contenedores, \textit{Apache Airflow} necesita acceso al \textit{Docker socket}. No obstante, conceder acceso a este \textit{socker} significa otorgar permisos de administrador local sobre la propia máquina en la que se despliega. Para solventar este problema y securizar el \textit{socket}, se ha desplegado un \textit{proxy}~\cite{dockerProxy} también \textit{dockerizado} y configurable que permite el paso de peticiones benignas y bloquea las malignas.

\subsection{Integración y flujos de datos}

La orquestación de los procesos y comunicación entre los numerosos componentes del proyecto ha requerido la creación de distintos flujos de datos utilizando diversas tareas y configuraciones.

Teniendo en cuenta la cantidad de fuentes de datos seleccionadas finalmente para el proyecto, se han diseñado dos \textit{data pipelines}. Un flujo de datos con un \textit{DAG} (\textit{Directed Acyclic Graph}) normal para los datos provenientes de la \textit{API} de Twitter y otro flujo de datos que genera de manera dinámica los \textit{DAGs} necesarios para todos los subconjuntos de datos del \textit{dataset} explicado en anteriores apartados. No obstante, en ambos flujos de datos han intervenido las mismas tareas y en el mismo orden de ejecución, como se puede observar en la \autoref{fig:airflow-data-pipeline}.

\imagen{airflow-data-pipeline}{Tareas de la \textit{data pipeline} sobre el \textit{dataset} de demostración}

\begin{itemize}
    \item La primera tarea (\texttt{sync\_data}) es un actuador que se encarga de ejecutar la sincronización de los datos en Airbyte. Para ello, es necesario establecer previamente la conexión con la herramienta de extracción de datos desde la configuración de Airflow y especificar el identificador de la conexión a ejecutar.

    \item La segunda tarea (\texttt{sensor\_sync\_data}) es un sensor que se encarga de recibir la señal con el estado de la sincronización cuando Airbyte finaliza la extracción de datos. Según el estado de la señal, finalización satisfactoria o no, termina la \textit{pipeline} o continúa con la siguiente tarea.

    \item La tercera tarea (\texttt{process\_data}) es un actuador que se encarga de realizar el procesamiento de los datos mediante Apache Spark. Los parámetros de configuración necesitan los datos de conexión a la base de datos MongoDB, junto al término de búsqueda empleado para las consultas de extracción y en qué colección se están almacenando estos datos en bruto. Esta tarea levanta un contenedor \textit{Docker} con Apache Spark para realizar sus operaciones, que posteriormente es eliminado para liberar los recursos ocupados. Durante la ejecución de esta fase se puede acceder a la interfaz web de monitorización que provee Spark.

    \item La cuarta tarea (\texttt{nlp\_inference}) es otro actuador que se encarga de realizar la inferencia de los modelos \textit{NLP} (\textit{Natural Language Processing}). Los parámetros de configuración necesitan los datos de conexión a las bases de datos MongoDB y ClickHouse, junto al término de búsqueda empleado para las consultas de extracción. Esta tarea levanta un contenedor \textit{Docker} con la librería \textit{HuggingFace Transformers} y emplea los modelos descritos en la sección anterior para realizar la inferencia de los datos. Tras su finalización, los datos son agregados en ClickHouse y el contenedor eliminado para liberar los recursos utilizados.
\end{itemize}


\section{Interfaz web de acceso centralizado}

Como se ha explicado en los apartados anteriores, la mayoría de los componentes utilizados para crear esta plataforma poseen una interfaz web que permite monitorizar y gestionar el servicio correspondiente.

La modularidad que ofrece esta arquitectura invita a posibles extensiones de funcionalidades o a la incorporación de distintas herramientas o nuevos servicios. Esto implica también la posibilidad de que haya más interfaces web a las que acceder según con qué parte de la plataforma se quiera trabajar, en caso de que se tenga los permisos suficientes para ello.

Por estas razones, se ha decidido crear un punto único de acceso centralizado a todas las interfaces de gestión y monitorización en forma de página web. De esta manera, se agiliza el acceso al resto de interfaces que ofrece la plataforma actualmente en caso de que se necesite trabajar de manera paralela en varios puntos de la misma.

El desarrollo de la página web (véase la \autoref{fig:web-app}) se ha realizado mediante el \textit{framework Flask} con el objetivo de que esta interfaz sea una aplicación web ligera con las funcionalidades básicas y necesarias. El diseño se ha realizado a medida, \textit{responsive} (adaptable a cualquier dispositivo) y teniendo en cuenta un estilo minimalista acorde a las necesidades de un punto web de acceso centralizado.

\imagen{web-app}{Aplicación web desarrollada como punto de acceso centralizado}
