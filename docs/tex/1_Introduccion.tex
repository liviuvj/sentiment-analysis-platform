\capitulo{1}{Introducción}

Cada día se genera una inmensa cantidad de datos en Internet, esto supone una fuente de información muy útil para numerosos casos de uso. Las redes sociales ofrecen de manera pública la gran mayoría de estos en forma de opiniones de personas, lo que invita a su estudio mediante el uso de técnicas como el análisis de sentimientos.

No obstante, para llegar a obtener conocimiento a partir de todos esos datos en bruto, es necesaria la capacidad de explotarlos de manera eficiente. Este proyecto tomará como objetivo la creación de una plataforma \textit{big data} de código abierto para el análisis de sentimientos. 

En los siguientes apartados se describe el planteamiento del proyecto, la estructura de la memoria y los materiales adjuntos a la misma.

\section{Estructura de la memoria}

La memoria está organizada de la siguiente manera:

\begin{itemize}
    \item \textbf{Introducción:} La estructuración de la memoria, los materiales adjuntos a la misma y el planteamiento del proyecto.
    \item \textbf{Objetivos del proyecto:} Descripción de los objetivos definidos inicialmente que se han intentado cumplir en su totalidad.
    \item \textbf{Conceptos teóricos:} Explicación de temas a tener en cuenta para el desarrollo del proyecto y de los modelos de \textit{Deep Learning} empleados.
    \item \textbf{Técnicas y herramientas:} Que se han utilizado para la realización del proyecto y facilitar algunas labores de programación, desarrollo o gestión.
    \item \textbf{Aspectos relevantes del desarrollo del proyecto:} Se entra en detalle en los puntos clave que han tenido mayor importancia para la correcta realización de este proyecto. En esta sección se expondrán los desafíos presentados y los pasos llevados a cabo para solventarlos.
    \item \textbf{Trabajos relacionados:} Aplicaciones existentes relacionadas con el proyecto y <<estado del arte>> sobre técnicas de procesamiento de lenguaje natural.
    \item \textbf{Conclusiones y líneas de trabajo futuras:} Deducciones y resultados obtenidos a lo largo del desarrollo de este Trabajo de Fin de Máster y posibles mejoras posteriores.
\end{itemize}

Junto a la memoria, se aportan también los siguientes anexos:

\begin{itemize}
    \item \textbf{Plan de proyecto software:} La planificación temporal del proyecto y estudio de la viabilidad económica y legal del mismo.
    \item \textbf{Especificación de requisitos:} Detalle de los requisitos técnicos, explicados los objetivos generales y el catálogo de requisitos.
    \item \textbf{Especificación de diseño:} Explicación de la arquitectura de la plataforma desarrollada y el diseño de datos.
    \item \textbf{Documentación técnica de programación:} La documentación técnica para la instalación, despliegue e integración de futuras funcionalidades.
    \item \textbf{Documentación de usuario:} Guía para utilizar el proyecto de manera segura y correcta.
\end{itemize}

\section{Materiales adjuntos}

Además de la memoria y de los anexos, se proporcionan también los siguientes materiales disponibles de manera \textit{online}:

\begin{itemize}
    \item Repositorio del proyecto en \textit{GitHub}.\\
        \url{https://github.com/liviuvj/sentiment-analysis-platform}
    
    \item Contenedor \textit{Docker} con las mejoras desarrolladas del conector para Twitter de  Airbyte.\\
        \url{https://hub.docker.com/r/liviuvj/airbyte-source-twitter/tags}

    \item Vídeos sobre el funcionamiento de la plataforma:
    \begin{itemize}
        \item Funcionamiento general de la plataforma.\\
            \url{...}
        \item Explicación detallada de la plataforma.\\
            \url{...}
    \end{itemize}

    \item Los datos extraídos de \textit{API} mediante la herramienta Airbyte.\\
        \url{...}

    \item El análisis realizado en \textit{Google Colaboratory}.\\
        \url{https://colab.research.google.com/drive/1d_obU9idFqjsDi7ezeFs1CORxTUAgh7V#offline=true&sandboxMode=true}

    \item Las particiones creadas a partir del conjuntos de datos utilizado.
    \begin{itemize}
        \item \textit{dataset\_movie}.\\
            \url{https://drive.google.com/file/d/1doLbhxFP5y4TRoMUpx6kgaIR3MUFVDVX/view}
        \item \textit{dataset\_got}.\\
            \url{...}
        \item \textit{dataset\_season8}.\\
            \url{https://drive.google.com/file/d/1tSA5bGkBGfgVWdltxLEEfd_NNm0qnUYs/view}
        \item \textit{dataset\_daenerys}.\\
            \url{https://drive.google.com/file/d/1hL3eh3K2lKtMNEkaG2JSgNSXjoNtHyTG/view}
        \item \textit{dataset\_jon}.\\
            \url{https://drive.google.com/file/d/1Uji4IajDAYlAj3yhQdQ9B1NcSFg-pqrG/view}
    \end{itemize}
    
\end{itemize}

\section{Planteamiento del proyecto}

El proyecto está formado por varios componentes, todos ellos de código abierto, que se encargan de diferentes tareas. Todos los componentes de la plataforma están completamente <<\textit{dockerizados}>>, es decir, empaquetados y aislados en contenedores \textit{Docker} independientes y desplegables en cualquier máquina.

Con estas decisiones de diseño se elimina la dependencia entre componentes, creando así una arquitectura modular para la plataforma. Por lo que resulta adaptable a distintos casos de uso o a la utilización de distintos componentes a los integrados actualmente para cada parte del proyecto, creando así una plataforma configurable según las necesidades que se planteen. En la \autoref{fig:platform-architecture-basic} se puede observar la arquitectura planteada del proyecto.

\imagen{platform-architecture-basic}{Vista básica de la arquitectura del proyecto}

El primero es \textit{Airbyte}, una herramienta que permite la extracción de datos desde distintos orígenes, como \textit{APIs}, bases de datos o archivos disponibles en la \textit{web}. Se ha utilizado para la ingestión de datos desde \textit{API} y desde \textit{Google Drive}. También se ha desarrollado y mejorado el conector base de Twitter que presenta la herramienta y se ha publicado en el repositorio oficial de la misma, quedando así disponible su uso para la comunidad.

El segundo es \textit{MongoDB}, una base de datos no relacional orientada a documentos. Se encarga de almacenar los datos en bruto de todas las fuentes de datos, actuando de esta manera como un \textit{Data Lake}\footnote{Un \textit{Data Lake} es un repositorio centralizado que almacena una gran cantidad y variedad de datos en su formato original. Esto permite a los usuarios acceder a los datos de forma flexible y rápida. Además de facilitar el descubrimiento de patrones, tendencias e \textit{insights}~\cite{awsDataLake}.} para la plataforma.

El tercero es \textit{Apache Spark}, un \textit{framework} de computación distribuida que permite procesar grandes volúmenes de datos de forma paralela y eficiente. Su labor en el proyecto se centra en el preprocesamiento de los datos extraídos, tanto su limpieza y filtrado como el enriquecimiento mediante <<metadatos>>.

El cuarto es \textit{HuggingFace Transformers}, una librería del lenguaje de programación Python que ofrece modelos preentrenados de procesamiento del lenguaje natural (\textit{NLP, Natural Language Processing}) basados en \textit{Deep Neural Networks}. Esta librería se utiliza para aplicar técnicas de \textit{NLP} sobre los datos preprocesados, desde la clasificación de sentimientos hasta la detección de entidades. 

El quinto es \textit{ClickHouse}, una base de datos columnar orientada al procesamiento analítico de datos en línea (\textit{OLAP, On-Line Analytical Processing}) que permite realizar consultas rápidas y complejas sobre los datos. Su labor es ejercer de \textit{Data Warehouse}\footnote{Un \textit{Data Warehouse} es un sistema de almacenamiento de datos que integra información de diversas fuentes y que facilita la ejecución de consultas y análisis complejos. La diferencia con una base de datos tradicional reside en que está diseñado para facilitar el procesamiento de datos analítico (\textit{OLAP}), no transaccional~\cite{sasDataWarehouse}.} del proyecto, puesto que almacenará los datos finales enriquecidos tras las etapas anteriores.

El sexto es \textit{Apache Superset}, una herramienta de visualización de datos que permite crear cuadros de mando interactivos y personalizados con gráficos, mapas o tablas. Su labor consistirá en permitir a los usuarios la creación y visualización de \textit{dashboards} para explotar la información obtenida hasta el momento, como exploración de datos, métricas resultantes y análisis de sentimientos.

El séptimo es \textit{Apache Airflow}, una plataforma de orquestación de flujos de trabajo que permite automatizar y programar tareas. Este componente se ha utilizado para gestionar las \textit{data pipelines} diseñadas y organizar las interacciones entre los demás componentes, como el despliegue dinámico de \textit{Spark} y \textit{Transformers} solamente cuando resulte necesario, permitiendo así la optimización de los recursos.

El octavo es una interfaz web como desarrollo propio y a medida para este proyecto. Tiene como objetivo servir de punto de acceso centralizado a las demás aplicaciones web que ofrecen las distintas herramientas empleadas.

El planteamiento de esta plataforma ofrece una solución \textit{big data} modular e integral para el análisis de sentimientos. La combinación de los componentes descritos ha sido elegida por formar una plataforma con tecnologías modernas, flexibles y altamente escalables que permitan explotar de manera eficiente los datos y obtener el valor esperado.
